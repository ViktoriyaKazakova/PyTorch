{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Курсовой проект по курсу \"Фреймворк PyTorch для разработки искусственных нейронных сетей\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно написать приложение, которое будет считывать и выводить кадры с веб-камеры. В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре. После этого, человек показывает жесты руками, а алгоритм должен считать их и определенным образом реагировать на эти жесты. \n",
    "На то, как система будет реагировать на определенные жесты - выбор за вами. Например, на определенный жест (жест пис), система будет здороваться с человеком. На другой, будет делать скриншот экрана. И т.д.\n",
    "Для распознавания жестов, вам надо будет скачать датасет  https://www.kaggle.com/gti-upm/leapgestrecog, разработать модель для обучения и обучить эту модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "from matplotlib import image\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', 'leapGestRecog']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './leapGestRecog'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_classname = {0: 'down', \n",
    "                  1: 'c', \n",
    "                  2: 'ok', \n",
    "                  3: 'l', \n",
    "                  4: 'palm_moved', \n",
    "                  5: 'fist', \n",
    "                  6: 'index', \n",
    "                  7: 'palm', \n",
    "                  8: 'thumb', \n",
    "                  9: 'fist_moved'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "imagepaths = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('./leapGestRecog'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        if path.endswith(\"png\"):\n",
    "            imagepaths.append(path)\n",
    "\n",
    "print(len(imagepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded:  19999\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for path in imagepaths[:19999]:\n",
    "    img = cv2.imread(path)\n",
    "    X.append(img) \n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "print(\"Images loaded: \", len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = tt.Compose([tt.Grayscale(num_output_channels=1), # Картинки чернобелые\n",
    "                         \n",
    "                         # Настройки для расширения датасета\n",
    "                         tt.RandomHorizontalFlip(),           # Случайные повороты на 90 градусов\n",
    "                         tt.RandomRotation(30),               # Случайные повороты на 30 градусов\n",
    "                         tt.ToTensor()])                      # Приведение к тензору\n",
    "test_transforms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scandir: embedded null character in path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b8a9e492d82f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: scandir: embedded null character in path"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(X_train, train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scandir: embedded null character in path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-b4acd6c58105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dataset\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_transforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Prog\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[1;34m(self, dir)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: scandir: embedded null character in path"
     ]
    }
   ],
   "source": [
    "test_dataset  = ImageFolder(X_test, test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала при исполнении train_dataset = ImageFolder(X_train, train_transforms) и test_dataset  = ImageFolder(X_test, test_transforms) выходила другая ошибка, Memory error без комментариев. Потом test_dataset  = ImageFolder(X_test, test_transforms) все же запустилась, и я подумала что для трейна не хватает памяти.  После увеличения файлов подкачки (к сожалению нет возможности запустить ноутбук на более мощной машине) ошибка изменилась, но и тест перестал работать. Гуглила ошибку, информация что дело в кодировке или слешах, но тут как будто не подходит. На этом месте работа тормозиться к сожалению, далее прилагаю дальнейший код, как это бы выглядело если бы работало :) Там вроде все нормально должно быть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "test_dataloader = DeviceDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = self.conv_block(in_channels, 128)\n",
    "        self.conv2 = self.conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(self.conv_block(128, 128), self.conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = self.conv_block(128, 256)\n",
    "        self.conv4 = self.conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(self.conv_block(256, 256), self.conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = self.conv_block(256, 512)\n",
    "        self.conv6 = self.conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(self.conv_block(512, 512), self.conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block(in_channels, out_channels, pool=False):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "                  nn.BatchNorm2d(out_channels), \n",
    "                  nn.ELU(inplace=True)]\n",
    "        if pool: layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(ResNet(1, len(digit_to_classname)), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если работаете на гпу, очищаем весь кэш\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "max_lr = 0.008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), max_lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader)\n",
    "print(f'{epochs} epochs, {total_steps} total_steps per epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    time1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_loss.append(loss.item())\n",
    "        if (batch_idx+1) % 10000 == 9999:\n",
    "            print(f'Train Epoch: {epoch+1}, Loss: {running_loss/10000}')\n",
    "            time2 = time.time()\n",
    "            print(f'Spend time for 10000 images: {time2 - time1} sec')\n",
    "            time1 = time.time()\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [np.mean(loss) for loss in epoch_losses]\n",
    "plt.plot(losses, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('losses')\n",
    "plt.title('losses vs. No. of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model_state_50_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ResNet(1, len(classes_train)).to(device)\n",
    "net.load_state_dict(torch.load('./model_state_50_epoch.pth'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект для считывания потока с веб-камеры(обычно вебкамера идет под номером 0. иногда 1)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "# Класс детектирования и обработки лица с веб-камеры \n",
    "class HandDetector(object):\n",
    "\n",
    "    def __init__(self, mtcnn):\n",
    "        self.mtcnn = mtcnn\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.emodel = ResNet(1, 10).to(self.device)\n",
    "        self.emodel.load_state_dict(torch.load('./model_state_50_epoch.pth'))\n",
    "        self.emodel.eval()\n",
    "\n",
    "    # Функция рисования найденных параметров на кадре\n",
    "    def _draw(self, frame, boxes, probs, gest):\n",
    "        try:\n",
    "            for box, prob, ld in zip(boxes, probs):\n",
    "                # Рисуем обрамляющий прямоугольник \n",
    "                cv2.rectangle(frame,\n",
    "                              (box[0], box[1]),\n",
    "                              (box[2], box[3]),\n",
    "                              (0, 0, 255),\n",
    "                              thickness=2)\n",
    "\n",
    "                # пишем на кадре какой жест распознан\n",
    "                cv2.putText(frame, \n",
    "                    gest, (box[2], box[3]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        except:\n",
    "            print('Something wrong im draw function!')\n",
    "\n",
    "        return frame\n",
    "    \n",
    "    # Функция для вырезания лиц с кадра\n",
    "    @staticmethod\n",
    "    def crop_faces(frame, boxes):\n",
    "        faces = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            faces.append(frame[int(box[1]):int(box[3]), \n",
    "                int(box[0]):int(box[2])])\n",
    "        return faces\n",
    "    \n",
    "    @staticmethod\n",
    "    def digit_to_classname(digit):\n",
    "        classes = i{0: 'down', \n",
    "                  1: 'c', \n",
    "                  2: 'ok', \n",
    "                  3: 'l', \n",
    "                  4: 'palm_moved', \n",
    "                  5: 'fist', \n",
    "                  6: 'index', \n",
    "                  7: 'palm', \n",
    "                  8: 'thumb', \n",
    "                  9: 'fist_moved'}\n",
    "            \n",
    "            return classes[digit]\n",
    "       \n",
    "    # Функция в которой будет происходить процесс считывания и обработки каждого кадра\n",
    "    def run(self):              \n",
    "        # Заходим в бесконечный цикл\n",
    "        while True:\n",
    "            # Считываем каждый новый кадр - frame\n",
    "            # ret - логическая переменая. Смысл - считали ли мы кадр с потока или нет\n",
    "            ret, frame = cap.read()\n",
    "            try:\n",
    "                # детектируем расположение лица на кадре, вероятности на сколько это лицо\n",
    "                # и особенные точки лица\n",
    "                boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=False)\n",
    "            if probs > 0.5:\n",
    "                \n",
    "                # Меняем размер изображения лица для входа в нейронную сеть\n",
    "                face = cv2.resize(frame, (128, 128))\n",
    "                # Превращаем в 1-канальное серое изображение\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                # Превращаем numpy-картинку вырезанного лица в pytorch-тензор\n",
    "                torch_face = torch.from_numpy(face).unsqueeze(0).to(self.device).float()\n",
    "                # Загужаем наш тензор лица в нейронную сеть и получаем предсказание\n",
    "                gest = self.gmodel(torch_face[None, ...])\n",
    "                # Интерпретируем предсказание как строку нашей эмоции\n",
    "                gest = self.digit_to_classname(gest.argmax().item())\n",
    "\n",
    "                # Рисуем на кадре\n",
    "                self._draw(frame, boxes, probs, gest)\n",
    "                \n",
    "            else:\n",
    "                    print('Face not detected')\n",
    "\n",
    "            except:\n",
    "                print('Something wrong im main cycle!')\n",
    "\n",
    "\n",
    "            # Показываем кадр в окне, и назвываем его(окно) - 'Face Detection'\n",
    "            cv2.imshow('Face Detection', frame)\n",
    "            \n",
    "            # Функция, которая проверяет нажатие на клавишу 'q'\n",
    "            # Если нажатие произошло - выход из цикла. Конец работы приложения\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # Очищаем все объекты opencv, что мы создали\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "# Загружаем мтцнн\n",
    "mtcnn = MTCNN(keep_all=True, device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "# Создаем объект нашего класса приложения\n",
    "fcd = FaceDetector(mtcnn)\n",
    "# Запускаем\n",
    "fcd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
